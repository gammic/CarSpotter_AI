{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9730f53f",
    "outputId": "e7f81727-e10c-4898-ef26-6e8be15955be"
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# 1. Installazione libreria\n",
    "!pip install bing-image-downloader\n",
    "\n",
    "from bing_image_downloader import downloader\n",
    "import os\n",
    "import shutil\n",
    "import signal\n",
    "\n",
    "# 2. Auto da riconoscere\n",
    "ricerche = {\n",
    "    \"\"\"\"Fiat_Panda\": [\n",
    "        #\"Fiat Panda 2018\",\n",
    "        \"Fiat Panda 2020\",\n",
    "        \"Fiat Panda 3 serie\",\n",
    "        \"Fiat Panda usata\",\n",
    "    ],\n",
    "    \"Fiat_500\": [\n",
    "        #\"Fiat 500 usata 2018 strada\",\n",
    "        #\"Fiat 500 parcheggiata città\",\n",
    "        \"Fiat 500 2017 usata\"\n",
    "    ],\"\"\"\n",
    "    \"Tesla_Model_3\": [\n",
    "        \"Tesla Model 3 usata\",\n",
    "        #\"Tesla Model 3 charging street\",\n",
    "        #\"Tesla Model 3 dirty\"\n",
    "    ],\n",
    "    \"Dacia_Duster\": [\n",
    "        #\"Dacia Duster 2019 offroad\",\n",
    "        \"Dacia Duster usata\",\n",
    "        #\"Dacia Duster 2020 strada\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 3. Scarica le immagini automaticamente\n",
    "for cartella, keywords in ricerche.items():\n",
    "    for key in keywords:\n",
    "        print(f\"Scaricando: {key}...\")\n",
    "        downloader.download(\n",
    "            key,\n",
    "            limit=30,\n",
    "            output_dir=\"/content/drive/MyDrive/PM_project/dataset_auto\",\n",
    "            adult_filter_off=True,\n",
    "            force_replace=False,\n",
    "            timeout=10,\n",
    "            verbose=True\n",
    "        )\n",
    "        print(f\"{key} scaricata\")"
   ],
   "metadata": {
    "id": "hNWGKu6sdNcV"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import uuid\n",
    "from PIL import Image\n",
    "\n",
    "# Configurazione\n",
    "dataset_path = \"/content/drive/MyDrive/PM_project/dataset_auto\"\n",
    "\n",
    "print(f\"--- Inizio pulizia e rinomina in {dataset_path} ---\")\n",
    "deleted_count = 0\n",
    "renamed_count = 0\n",
    "\n",
    "# Scansiona tutte le sottocartelle\n",
    "for folder in os.listdir(dataset_path):\n",
    "    folder_path = os.path.join(dataset_path, folder)\n",
    "\n",
    "    if os.path.isdir(folder_path):\n",
    "        print(f\"\\nElaborazione cartella: {folder}\")\n",
    "\n",
    "        valid_temp_files = []\n",
    "\n",
    "        # --- FASE 1: CONTROLLO E RINOMINA TEMPORANEA ---\n",
    "        for filename in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "            if os.path.isdir(file_path):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                with Image.open(file_path) as img:\n",
    "                    img.load() \n",
    "                    print(f\"Analisi file {filename}\")\n",
    "\n",
    "                    # Controllo formato\n",
    "                    if img.format not in ['JPEG', 'PNG', 'GIF', 'BMP']:\n",
    "                        print(f\"Formato errato ({img.format}): {filename} -> ELIMINATO\")\n",
    "                        os.remove(file_path)\n",
    "                        deleted_count += 1\n",
    "                    else:\n",
    "                        # Se è valida: rinomina temporanea per evitare conflitti\n",
    "                        # Usato UUID per avere un nome univoco sicuro\n",
    "                        ext = os.path.splitext(filename)[1].lower()\n",
    "                        if not ext: # Se non ha estensione, viene dedotta dal formato\n",
    "                            ext = f\".{img.format.lower()}\"\n",
    "                            if ext == \".jpeg\": ext = \".jpg\"\n",
    "\n",
    "                        temp_name = f\"temp_{uuid.uuid4()}{ext}\"\n",
    "                        temp_path = os.path.join(folder_path, temp_name)\n",
    "\n",
    "                        os.rename(file_path, temp_path)\n",
    "                        valid_temp_files.append(temp_path)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"File corrotto: {filename} -> ELIMINATO\")\n",
    "                if os.path.exists(file_path):\n",
    "                    os.remove(file_path)\n",
    "                    deleted_count += 1\n",
    "\n",
    "        # --- FASE 2: RINOMINA FINALE INCREMENTALE (Image_1, Image_2...) ---\n",
    "        valid_temp_files.sort()\n",
    "        total_files = len(valid_temp_files)\n",
    "\n",
    "        for i, temp_path in enumerate(valid_temp_files, 1):\n",
    "            # Recupera estensione dal file temporaneo\n",
    "            print(f\"Riordinamento file {i} su {total_files}\")\n",
    "            ext = os.path.splitext(temp_path)[1]\n",
    "\n",
    "            # Crea il nuovo nome finale: Image_1.jpg, Image_2.jpg...\n",
    "            final_name = f\"Image_{i}{ext}\"\n",
    "            final_path = os.path.join(folder_path, final_name)\n",
    "\n",
    "            os.rename(temp_path, final_path)\n",
    "            renamed_count += 1\n",
    "\n",
    "        print(f\"Rinominate {len(valid_temp_files)} immagini in sequenza.\")\n",
    "\n",
    "# Rimozionr checkpoint di Colab\n",
    "checkpoint_path = os.path.join(dataset_path, '.ipynb_checkpoints')\n",
    "if os.path.exists(checkpoint_path):\n",
    "    import shutil\n",
    "    shutil.rmtree(checkpoint_path)\n",
    "\n",
    "print(f\"\\nOPERAZIONE COMPLETATA!\")\n",
    "print(f\"File eliminati: {deleted_count}\")\n",
    "print(f\"File rinominati e ordinati: {renamed_count}\")"
   ],
   "metadata": {
    "id": "dPYB0ns6_vov",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b1e83a6f-411d-43a9-d4ed-b2338684d655"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# --- CONFIGURAZIONE ---\n",
    "DATASET_DIR = \"/content/drive/MyDrive/PM_project/dataset_auto\"\n",
    "IMG_SIZE = 224      \n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20         \n",
    "NUM_CLASSES = 4\n",
    "\n",
    "print(f\"Analizzando le GPU disponibili...\")\n",
    "if not tf.config.list_physical_devices('GPU'):\n",
    "    print(\"ATTENZIONE: Nessuna GPU rilevata\")\n",
    "else:\n",
    "    print(\"GPU Rilevata! Il training sarà veloce.\")\n",
    "\n",
    "# --- 1. CARICAMENTO DATI ---\n",
    "print(\"\\n--- Caricamento Dataset ---\")\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    DATASET_DIR,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    crop_to_aspect_ratio=True\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    DATASET_DIR,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    crop_to_aspect_ratio=True\n",
    ")\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "print(f\"Classi trovate: {class_names}\")\n",
    "\n",
    "# Pre-processing per MobileNetV2 (porta i valori da 0-255 a -1 a 1)\n",
    "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "\n",
    "train_ds = train_ds.map(lambda x, y: (preprocess_input(x), y))\n",
    "val_ds = val_ds.map(lambda x, y: (preprocess_input(x), y))\n",
    "\n",
    "# Ottimizzazione della cache per velocizzare il caricamento\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# --- 2. DATA AUGMENTATION ---\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "  layers.RandomFlip(\"horizontal\"),\n",
    "  layers.RandomRotation(0.1),\n",
    "  layers.RandomContrast(0.2),\n",
    "  layers.RandomTranslation(0.1, 0.1)\n",
    "])\n",
    "\n",
    "# --- 3. COSTRUZIONE MODELLO (MobileNetV2) ---\n",
    "print(\"\\n--- Costruzione Modello ---\")\n",
    "\n",
    "# Scaricamento modello pre-addestrato (MobileNetV2)\n",
    "base_model = tf.keras.applications.MobileNetV2(\n",
    "    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    include_top=False,\n",
    "    weights='imagenet'\n",
    ")\n",
    "\n",
    "# Congelamento della base\n",
    "base_model.trainable = False\n",
    "\n",
    "inputs = tf.keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "x = data_augmentation(inputs) \n",
    "x = base_model(x, training=False)\n",
    "x = layers.GlobalAveragePooling2D()(x) \n",
    "x = layers.Dropout(0.2)(x)\n",
    "\n",
    "# STRATO FINALE:\n",
    "outputs = layers.Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "\n",
    "model = models.Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# --- 4. TRAINING ---\n",
    "print(\"\\n--- Inizio Training... ---\")\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS\n",
    ")\n",
    "\n",
    "# --- 5. GRAFICI ---\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Loss')\n",
    "plt.show()\n",
    "\n",
    "# --- 6. ESPORTAZIONE PER ANDROID ---\n",
    "print(\"\\n--- Conversione in TFLite per Android ---\")\n",
    "\n",
    "# Conversione del modello\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Salvataggio del file .tflite\n",
    "with open('/content/drive/MyDrive/PM_project/car_recognizer.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "# Salvataggio delle etichette (labels.txt)\n",
    "with open('/content/drive/MyDrive/PM_project/labels.txt', 'w') as f:\n",
    "    for name in class_names:\n",
    "        f.write(name + '\\n')\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WyDyIY1o-mpp",
    "outputId": "5f994f8b-d3cb-4b96-9fd7-37d2c2647b12"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# --- FASE 2: FINE TUNING ---\n",
    "print(\"\\n--- Inizio Fine Tuning ---\")\n",
    "\n",
    "# 1. Scongelamento del cervello di MobileNet\n",
    "base_model.trainable = True\n",
    "\n",
    "# Scongelati solo gli ultimi 50 strati (quelli che vedono i dettagli complessi)\n",
    "# lasciando congelati i primi 100 (che vedono linee e cerchi base).\n",
    "print(f\"Numero totale strati nella base: {len(base_model.layers)}\")\n",
    "\n",
    "fine_tune_at = 100\n",
    "\n",
    "# Congela tutti i layer prima del numero 100\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# 2. Ricompiliamo con un Learning Rate molto basso\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,               \n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# 3. Addestriamo per altri 10 giri (Epoche)\n",
    "total_epochs = 40\n",
    "\n",
    "print(f\"\\n--- Ripartiamo da dove eravamo rimasti per altri 20 giri ---\")\n",
    "\n",
    "history_fine = model.fit(train_ds,\n",
    "                         epochs=total_epochs,\n",
    "                         initial_epoch=history.epoch[-1],\n",
    "                         validation_data=val_ds,\n",
    "                         callbacks=[early_stopping]\n",
    "                         )\n",
    "\n",
    "# --- 4. NUOVI GRAFICI E SALVATAGGIO ---\n",
    "\n",
    "acc += history_fine.history['accuracy']\n",
    "val_acc += history_fine.history['val_accuracy']\n",
    "loss += history_fine.history['loss']\n",
    "val_loss += history_fine.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.plot([EPOCHS-1,EPOCHS-1], plt.ylim(), label='Inizio Fine Tuning')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Accuracy dopo Fine Tuning')\n",
    "plt.show()\n",
    "\n",
    "# Modello fine tuned\n",
    "print(\"\\n--- Salvataggio Modello nuovo ---\")\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open('/content/drive/MyDrive/PM_project/car_recognizer_tuned.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "CbE1KTQcCn54",
    "outputId": "accf27e8-3bf9-4234-be19-d5006418067d"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Prendiamo tutte le immagini di validazione e le loro etichette reali\n",
    "print(\"Calcolo delle predizioni in corso...\")\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for images, labels in val_ds:\n",
    "\n",
    "    # Eseguiamo la predizione\n",
    "    preds = model.predict(images, verbose=0)\n",
    "    # Convertiamo le probabilità in numeri interi (l'indice della classe vincente)\n",
    "    preds_classes = np.argmax(preds, axis=1)\n",
    "\n",
    "    y_true.extend(labels.numpy())\n",
    "    y_pred.extend(preds_classes)\n",
    "\n",
    "# 2. Generiamo il Report Testuale (Precision, Recall, F1-Score)\n",
    "\n",
    "print(\"\\n--- PAGELLA PER CLASSE ---\")\n",
    "print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "\n",
    "# 3. Disegniamo la Matrice di Confusione (Grafico)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Cosa ha predetto il Modello')\n",
    "plt.ylabel('Cosa era REALMENTE')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "NvNyktecEgKm",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "70747900-6c2f-4ab4-bf2c-3baba4614998"
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
